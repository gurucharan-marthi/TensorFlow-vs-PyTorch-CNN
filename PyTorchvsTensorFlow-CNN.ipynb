{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LeNet-PyTorchvsTensorFlow.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMk7LJmt2ulH",
        "colab_type": "text"
      },
      "source": [
        "## Importing the Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e322RlzH2R1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline  \n",
        "\n",
        "#TensorFlow - Importing the Libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "#PyTorch - Importing the Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9rTPV4H25rq",
        "colab_type": "text"
      },
      "source": [
        "## Getting and Splitting the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVNN2pDa243P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#PyTorch - Getting and Splitting the Dataset\n",
        "transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
        "train_dataset_pytorch = torchvision.datasets.FashionMNIST(root='./data/',\n",
        "                                             train=True, \n",
        "                                             transform=transforms,\n",
        "                                             download=True)\n",
        "test_dataset_pytorch = torchvision.datasets.FashionMNIST(root='.data/',\n",
        "                                             train=False, \n",
        "                                             transform=transforms,\n",
        "                                             download=True)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rL8IvrV22at_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TensorFlow - Getting and Splitting the Dataset\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(train_images_tf, train_labels_tf), (test_images_tf, test_labels_tf) = fashion_mnist.load_data()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYy_7jSU3Cb8",
        "colab_type": "text"
      },
      "source": [
        "## Loading the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPxraJXR4vcq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "432f1548-6297-4e2a-eee2-7ed5944e5bf6"
      },
      "source": [
        "#PyTorch - Loading the Data\n",
        "def imshowPytorch(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset_pytorch,\n",
        "                                           batch_size=32, \n",
        "                                           shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset_pytorch,\n",
        "                                           batch_size=32, \n",
        "                                           shuffle=False)\n",
        "                                           \n",
        "data_iter = iter(train_loader)\n",
        "images, label = data_iter.next()\n",
        "imshowPytorch(torchvision.utils.make_grid(images[0]))\n",
        "print(label[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(9)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTfN4iJ64zJM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "b4a6ac9e-1e87-4a90-f112-f983d2f4b3e7"
      },
      "source": [
        "#TensorFlow - Loading the Data\n",
        "def imshowTensorFlow(img):\n",
        "  plt.imshow(img)\n",
        "imshowTensorFlow(train_images_tf[0])\n",
        "\n",
        "print(train_labels_tf[0])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NafLsOFa3p6E",
        "colab_type": "text"
      },
      "source": [
        "## Building the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gVZFRnG43hi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#PyTorch - Building the Model\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, num_of_class):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.cnn_model = nn.Sequential(\n",
        "            nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2))\n",
        "        self.fc_model = nn.Sequential(\n",
        "            nn.Linear(400,120),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(120,84),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.classifier = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn_model(x)\n",
        "        x = x.view(-1, 16*5*5)\n",
        "        x = self.fc_model(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp-7n9SI45Pc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TensorFlow - Building the Model\n",
        "modeltf = keras.Sequential([\n",
        "    keras.layers.Conv2D(input_shape=(28,28,1), filters=6, kernel_size=5, strides=1, padding=\"same\", activation=tf.nn.relu),\n",
        "    keras.layers.AveragePooling2D(pool_size=2, strides=2),\n",
        "    keras.layers.Conv2D(16, kernel_size=5, strides=1, padding=\"same\", activation=tf.nn.relu),\n",
        "    keras.layers.AveragePooling2D(pool_size=2, strides=2),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(120, activation=tf.nn.relu),\n",
        "    keras.layers.Dense(84, activation=tf.nn.relu),\n",
        "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxnA618i3w2c",
        "colab_type": "text"
      },
      "source": [
        "## Visualizing the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaiLEeVG46yV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "b4100b94-ebcb-4b41-e98d-828b0a85f336"
      },
      "source": [
        "#PyTorch - Visualizing the Model\n",
        "modelpy = NeuralNet(10)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optim = torch.optim.Adam(modelpy.parameters())\n",
        "\n",
        "modelpy"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NeuralNet(\n",
              "  (cnn_model): Sequential(\n",
              "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (1): ReLU()\n",
              "    (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
              "    (4): ReLU()\n",
              "    (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "  )\n",
              "  (fc_model): Sequential(\n",
              "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
              "    (3): ReLU()\n",
              "  )\n",
              "  (classifier): Linear(in_features=84, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VETxxaRe48St",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "4f4b9a12-265d-486b-d756-2e3838d3dc9b"
      },
      "source": [
        "#TensorFlow - Visualizing the Model\n",
        "modeltf.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "modeltf.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 28, 28, 6)         156       \n",
            "_________________________________________________________________\n",
            "average_pooling2d (AveragePo (None, 14, 14, 6)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 16)        2416      \n",
            "_________________________________________________________________\n",
            "average_pooling2d_1 (Average (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 120)               94200     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 84)                10164     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                850       \n",
            "=================================================================\n",
            "Total params: 107,786\n",
            "Trainable params: 107,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syYmAVtW3_E6",
        "colab_type": "text"
      },
      "source": [
        "## Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DG8M01yM4_eW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "a9e80a15-afe4-4d4f-fa17-c864ce6fe997"
      },
      "source": [
        "#PyTorch - Training the Model\n",
        "for e in range(30):\n",
        "    # define the loss value after the epoch\n",
        "    losss = 0.0\n",
        "    number_of_sub_epoch = 0\n",
        "    \n",
        "    # loop for every training batch (one epoch)\n",
        "    for images, labels in train_loader:\n",
        "        #create the output from the network\n",
        "        out = modelpy(images)\n",
        "        # count the loss function\n",
        "        loss = criterion(out, labels)\n",
        "        # in pytorch you have assign the zero for gradien in any sub epoch\n",
        "        optim.zero_grad()\n",
        "        # count the backpropagation\n",
        "        loss.backward()\n",
        "        # learning\n",
        "        optim.step()\n",
        "        # add new value to the main loss\n",
        "        losss += loss.item()\n",
        "        number_of_sub_epoch += 1\n",
        "    print(\"Epoch {}: Loss: {}\".format(e, losss / number_of_sub_epoch))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: Loss: 0.6227099574804306\n",
            "Epoch 1: Loss: 0.4179951703429222\n",
            "Epoch 2: Loss: 0.35394162097771964\n",
            "Epoch 3: Loss: 0.31588206316630046\n",
            "Epoch 4: Loss: 0.2895640591740608\n",
            "Epoch 5: Loss: 0.2687729511876901\n",
            "Epoch 6: Loss: 0.251606637219588\n",
            "Epoch 7: Loss: 0.23748337679803372\n",
            "Epoch 8: Loss: 0.22412400639454524\n",
            "Epoch 9: Loss: 0.21168595918913682\n",
            "Epoch 10: Loss: 0.20105701752901078\n",
            "Epoch 11: Loss: 0.19044897941052913\n",
            "Epoch 12: Loss: 0.18115745490640403\n",
            "Epoch 13: Loss: 0.17252162928283216\n",
            "Epoch 14: Loss: 0.16545736669301986\n",
            "Epoch 15: Loss: 0.15640809882879259\n",
            "Epoch 16: Loss: 0.1498145791510741\n",
            "Epoch 17: Loss: 0.14355199264089266\n",
            "Epoch 18: Loss: 0.1380430019915104\n",
            "Epoch 19: Loss: 0.13102651527822018\n",
            "Epoch 20: Loss: 0.12447771841287612\n",
            "Epoch 21: Loss: 0.12084027421437205\n",
            "Epoch 22: Loss: 0.11766422504087289\n",
            "Epoch 23: Loss: 0.10957689811562499\n",
            "Epoch 24: Loss: 0.10643648336914678\n",
            "Epoch 25: Loss: 0.10487306278670827\n",
            "Epoch 26: Loss: 0.09565478510657946\n",
            "Epoch 27: Loss: 0.09731809958498926\n",
            "Epoch 28: Loss: 0.09314991759688904\n",
            "Epoch 29: Loss: 0.0887936073852703\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyrdY6IO5DbW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "92ec2599-5917-47d8-cd34-46dcf77dbb09"
      },
      "source": [
        "#TensorFlow - Training the Model\n",
        "train_images_tensorflow = (train_images_tf / 255.0).reshape(train_images_tf.shape[0], 28, 28, 1)\n",
        "test_images_tensorflow = (test_images_tf / 255.0).reshape(test_images_tf.shape[0], 28, 28 ,1)\n",
        "train_labels_tensorflow=keras.utils.to_categorical(train_labels_tf)\n",
        "test_labels_tensorflow=keras.utils.to_categorical(test_labels_tf)\n",
        "modeltf.fit(train_images_tensorflow, train_labels_tensorflow, epochs=30, batch_size=32)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.5243 - accuracy: 0.8064\n",
            "Epoch 2/30\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3532 - accuracy: 0.8710\n",
            "Epoch 3/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2995 - accuracy: 0.8893\n",
            "Epoch 4/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2692 - accuracy: 0.8992\n",
            "Epoch 5/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2484 - accuracy: 0.9071\n",
            "Epoch 6/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2299 - accuracy: 0.9147\n",
            "Epoch 7/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2139 - accuracy: 0.9194\n",
            "Epoch 8/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2004 - accuracy: 0.9255\n",
            "Epoch 9/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1875 - accuracy: 0.9295\n",
            "Epoch 10/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1747 - accuracy: 0.9344\n",
            "Epoch 11/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1649 - accuracy: 0.9376\n",
            "Epoch 12/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1529 - accuracy: 0.9428\n",
            "Epoch 13/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1458 - accuracy: 0.9453\n",
            "Epoch 14/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1368 - accuracy: 0.9475\n",
            "Epoch 15/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1274 - accuracy: 0.9517\n",
            "Epoch 16/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1214 - accuracy: 0.9536\n",
            "Epoch 17/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1121 - accuracy: 0.9578\n",
            "Epoch 18/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1065 - accuracy: 0.9593\n",
            "Epoch 19/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0983 - accuracy: 0.9634\n",
            "Epoch 20/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0922 - accuracy: 0.9645\n",
            "Epoch 21/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0843 - accuracy: 0.9679\n",
            "Epoch 22/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0833 - accuracy: 0.9685\n",
            "Epoch 23/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0745 - accuracy: 0.9711\n",
            "Epoch 24/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0732 - accuracy: 0.9725\n",
            "Epoch 25/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0650 - accuracy: 0.9751\n",
            "Epoch 26/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0651 - accuracy: 0.9756\n",
            "Epoch 27/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0615 - accuracy: 0.9765\n",
            "Epoch 28/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0551 - accuracy: 0.9794\n",
            "Epoch 29/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0565 - accuracy: 0.9791\n",
            "Epoch 30/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0523 - accuracy: 0.9802\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fee201fc518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTjB8OrL4GpS",
        "colab_type": "text"
      },
      "source": [
        "## Comparing the Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZFF4wsf5FYX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "db8edf7e-6852-4aba-8ac6-67362372fa0b"
      },
      "source": [
        "#PyTorch - Comparing the Results\n",
        "correct = 0\n",
        "total = 0\n",
        "modelpy.eval()\n",
        "for images, labels in test_loader:\n",
        "    outputs = modelpy(images)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum()\n",
        "print('Test Accuracy of the model on the {} test images: {}% with PyTorch'.format(total, 100 * correct // total))\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy of the model on the 10000 test images: 89% with PyTorch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtejHS0BFFG9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "abd49e56-36cd-4bf9-e6b5-cabc80a997b0"
      },
      "source": [
        "#TensorFlow - Comparing the Results\n",
        "predictions = modeltf.predict(test_images_tensorflow)\n",
        "correct = 0\n",
        "for i, pred in enumerate(predictions):\n",
        "  if np.argmax(pred) == test_labels_tf[i]:\n",
        "    correct += 1\n",
        "print('Test Accuracy of the model on the {} test images: {}% with TensorFlow'.format(test_images_tf.shape[0],\n",
        "                                                                     100 * correct/test_images_tf.shape[0]))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy of the model on the 10000 test images: 91.34% with TensorFlow\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}